{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тележка с шестом - Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть тележка *cart*, которая может двигаться линейно вправо или влево. К ней прикреплен шест *pole*, который может вращаться. \n",
    "\n",
    "Другое название перевернутый маятний - *Inverted pendulum*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The CartPole problem is the Hello World of Reinforcement Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача управления - удерживать шест в вертикальном положении, применяя смещение тележки вправо и влево.\n",
    "\n",
    "Пространство действий *action space* состоит из двух действий:\n",
    "1) Сдвинуть тележку влево - обозначается 0\n",
    "2) Сдвинуть тележку вправо - обозначается 1\n",
    "\n",
    "Пространство состояний *states*:\n",
    "1) Положение тележки - обозначается $x$\n",
    "2) Скорость тележки - обозначается $\\dot x$\n",
    "3) Угол вращения шеста - обозначается $\\Theta$. Минимальное и максимальное значения равны -0,418 радиан (-24 градуса) и 0,418 (24 градуса).\n",
    "4) Угловая скорость шеста - обозначается $\\dot \\Theta$"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAGRCAIAAAAowmrZAAAgAElEQVR4Ae2d34sdR3aA+9V/xgjv6L8IgTCIsdYEFgkN0hLIjEeycBZ7bQstWEKxsFDklaxEs2KFVptBQRtb2WFAD0JMhILBmBkmD8HOSJCHYI/0khdHV48m7HZYlVxud9+uW7e7qvpU1WfEbk//qD7nOzX9TXdXdxcl/0EAAhCAAASyJFBkmTVJQwACEIAABEoUSCeAAAQgAIFMCaDATAtP2hCAAAQggALpAxCAAAQgkCkBFJhp4UkbAhCAAARQIH0AAhCAAAQyJYACMy08aUMAAhCAAAqkD0AAAhCAQKYEUGCmhSdtCEAAAhBAgfQBCEAAAhDIlAAKzLTwpA0BCEAAAiiQPgABCEAAApkSQIGZFp60IQABCEAABdIHIAABCEAgUwIoMNPCkzYEIAABCKBA+gAEIAABCGRKAAVmWnjShgAEIAABFEgfgAAEIACBTAmgwEwLT9oQgAAEIIAC6QMQgAAEIJApARSYaeFJGwIQgAAEUCB9AAIQgAAEMiWAAjMtPGlDAAIQgAAKpA9AwBeBP/7vf/3fp3/b+d8f//tffUVGuxCAwHMCKJCOAAFfBP7wP//x7fqRzv/+8J//7Csy2oUABJ4TQIF0BAj4IoACfZGlXQg4IoACHYGkGQg0CKDABhJmQEAWARQoqx5EkxIBFJhSNcklSQIoMMmykpQIAihQRBkIAgLtBFBgOxuWQKAfARTYjx9bQ8A7ARToHTE7yJYACsy29CQeCwEUGEuliDM+AigwvpoRcWYEUGBmBSfdgARQYEDY7AoCXQigwC7U2AYCNgRQoA0l1oHAgARQ4IDw2XXiBFBg4gUmvfgJoMD4a0gGUgmgQKmVIS4IvCCAAukKEPBFAAX6Iku7EHBEAAU6AkkzEGgQQIENJMyAgCwCKFBWPYgmJQIoMKVqkkuSBFBgkmUlKREEUKCIMhAEBNoJoMB2NiyBQD8CKLAfP7aGgHcCKNA7YnaQLQEUmG3pSTwWAigwlkoRZ3wEUGB8NSPizAigwMwKTroBCaDAgLDZFQS6EECBXaixDQRsCKBAG0qsA4EBCaDAAeGz68QJoMDEC0x68RNAgfHXkAykEkCBUitDXBB4QQAF0hUg4IsACvRFlnYh4IgACnQEkmYg0CCAAhtImAEBWQRQoKx6EE1KBFBgStUklyQJoMAky0pSIgigQBFlIAgItBNAge1sWAKBfgRQYD9+bA0B7wRQoHfE7CBbAigw29KTeCwEUGAslSLO+AigwPhqRsSZEUCBmRWcdAMSQIEBYbMrCHQhgAK7UGMbCNgQQIE2lFgHAgMSQIEDwmfXiRNAgYkXmPTiJ4AC468hGUglgAKlVoa4IPCCAAqkK0DAFwEU6Iss7ULAEQEU6AgkzUCgQQAFNpAwAwKyCKBAWfUgmpQIoMCUqkkuSRJAgUmWlaREEECBIspAEBBoJ4AC29mwBAL9CKDAfvzYGgLeCaBA74jZQbYEUGC2pSfxWAigwFgqRZzxEUCB8dWMiDMjgAIzKzjpBiSAAgPCZlcQ6EIABXahxjYQsCGAAm0osQ4EBiSAAgeEz64TJ4ACEy8w6cVPAAXGX0MykEoABUqtDHFB4AUBFEhXgIAvAijQF1nahYAjAijQEUiagUCDAApsIGEGBGQRQIGy6kE0KRFAgSlVk1ySJIACkywrSYkggAJFlIEgINBOAAW2s2EJBPoRQIH9+LE1BLwTQIHeEbODbAmgwGxLT+KxEECBsVSKOOMjgALjqxkRZ0YABWZWcNINSAAFBoTNriDQhQAK7EKNbSBgQwAF2lBiHQgMSAAFDgifXSdOAAUmXmDSi58ACoy/hmQglQAKlFoZ4oLACwIokK4AAV8EUKAvsrQLAUcEUKAjkDQDgQYBFNhAwgwIyCKAAmXVg2hSIoACU6omuSRJAAUmWVaSEkEABYooA0FAoJ0ACmxnwxII9COAAvvxY2sIeCeAAr0jZgfZEkCB2ZaexGMhgAJjqRRxxkcABcZXMyLOjAAKzKzgpBuQAAoMCJtdQaALARTYhRrbQMCGAAq0ocQ6EBiQAAocED67TpwACky8wKQXPwEUGH8NyUAqARQotTLEBYEXBFAgXQECvgigQF9kaRcCjgigQEcgaQYCDQIosIGEGRCQRQAFyqoH0aREIGEFvn/2XFEU8/P7U6qXq1yA44pkgHZQYADI7CJTAigwz8KjwIjqjgIjKhahRkYABTov2Pz8/qIo3j97znnLDht0q8AoUnZIL3BTKDAwcHaXEYGEFThUFTP0QYYph+xdKDAkbfaVFwEU6LzeGfogw5SddxtDgyjQAIdFEOhFAAX2wjdu4wx9kGHK4yrvax4K9EWWdiEQiwKPHjtevcF2/cbq7Oze4vl/CwuHP7691ixl7XbXV1/vqk2u31htrqzmfPX1rmrzs883q+vcvbfx7omTalFRFLOze6/fWP3q693qOnppc+LosePVNdumO+RYbcomSL1+DY6aXwvg49trCwuHdTqXLl9xnrKOhwkDARRogMMiCPQiEJ0Cv/p6t3pc1gfopmaaR3mlsYWFw23Irt9YrT1H8XQ0UmLQO6pO3L23oZuqzq9NN2PTW1UntIHsc1Sb2wepd9eEU5alDqCtwdnZvV98uaMbqaVZ/dEyZd0UEwYCKNAAh0UQ6EUgOgUuLBxWJ2Eq7a++3lVH86Io3j1xssqieZS/e29DHaarx/HqJkqu+jTx6WikdXvp8pWno5Fa+e69DT1fr6zb6XxVUBvIPseyLLsF2YRTVaD6W+H9s+d0yuqPg9rfB/1T1i0wYSCAAg1wWASBXgTiUuDs7N7Z2b21y3FlWeoDdPUCZvMo/3Q0MlwL1VdBtSAvXb6ilFltVuPWZ4e1eHoqcKocy7LsFmQTjlagQtS8tqwhV899FY3OKWuYTBgIoEADHBZBoBeBuBRYFEXz0KzyV0fh6vW3sUd5w7VQdYjXl0mfjkbKf83zPLVHLdTaI4CdfaCdap9j5yDHwtEB1DLSPUzZ8dLlK3pOFX7bVrWV+XFaAihwWmKsDwFbAnEpcHZ2b1ti+hxFX7sbe5Q3XAutXQXVDdZO8qoBqDOwWlQ9FVhrrbo7HZLOUc+ZNsixcLQC9Xlwde9lWSpEtQvOZVl2TrnWPj+OJYACx2JhJgQcEIhLgfoUrZn5Z59v1i5ajj3Kl2U59lqovgqqXdK2eXXXY4Xa2QfKQE5ynBjk2OxUAAYHqxWqp9pqR51TrsbJdBsBFNhGhvkQ6EsgLgU2D746f+0wfRVx7FG+LEs1v2YadTpVbb/tcK/3WJal3mn19lhnH0zco96dznHiJm1BjoWjWquRqebbtrvOKVcbZ7qNAApsI8N8CPQlkKECx566qUt8Wi16bEhVik3WTSf1uSrYJhi93+buJm5SVWA1O4MCDSm37Q4F6hr5mECBPqjSJgT+RCBDBTavhWq16Hts0yowpbNAFCjt0IACpVWEeNIhEJcCDdfo9L1APZRj7ImOqlztWqi6Clob5WHYXJd/7All51MidY7lKkdzkGOzazvJ0021rdA5Zd0yEwYCKNAAh0UQ6EUgLgUaRmrosZH6TG7sUV7BqqlLXQWtnslVnzXUA2SaoJMcEcpZYLPQw85BgcPyZ+8pE4hLgUVRjH1KXd+Bq57JGRRYvRb6xZc76rWf2p2q3vqRu+ZjcHoFNbi09jycOiWqRmLZgdQ51lQ5dg5yLJy2kzwdf9sKnVPWLTNhIIACDXBYBIFeBKJT4Pz8/pqr2t6QMvYor2GpE7iFhcPq9LGmMbVatxevqHPK+fn9el+WE1qB9jm25a73qNusncuOhdNmuFprzdPEzinrlpkwEECBBjgsgkAvAnEpUL08c35+v75oWX1HaO10bexRXsPS9w7VmVzbyaU6uBdF0faO0OowS9W4FudYreoAmhPKQFPlqBrpEORYOJ0V2DnlJgTmNAmgwCYT5kDADYG4FHj02PEvvtxR0lIPwuv/bfpm7FG+Sk23Y7jF2PbNBLVfbeJqs9X3Vuvw2hRb3VAbyD5HtXmHIMfC0QFUo6pOt63QOeVq40y3EUCBbWSYD4G+BKJToPo2gh78UhTF0WPHx6po7FG+ykufu9ROH6vrqOmxn+JrXo+tblj9oqHh9l51k6pgno5GNjlWN58qyLFwqgFUW9bT5hU6pKxbZsJAAAUa4LAIAr0IxKjAXgkL3tgsGMGBE5pfAijQL19az5kACpRTfRQopxaiIkGBospBMEkRQIFyyokC5dRCVCQoUFQ5CCYpAihQTjlRoJxaiIoEBYoqB8EkRQAFyiknCpRTC1GRoEBR5SCYpAigQDnlRIFyaiEqEhQoqhwEkxSBWBSYFHSSgcA0BFDgNLRYFwLTEECB09BiXQgMQAAFDgCdXWZCAAVmUmjSjJcACoy3dkQunQAKlF4h4sueAArMvgsAwBsBFOgNLQ1DwA0BFOiGI61AoEkgcwWORs9OnT4zGj1rkmEOBIQQQIFCCkEYCRLIXIEHDh4qiuLAwUM7Dx8lWF1SSoIACkyijCQhkkDOCjx1+oz+mNHMzJ7NrW2RJSKo3AmgwNx7APn7I5CtAldv3lL+W1u/o84Fi6JYW7/jDzUtQ6AbARTYjRtbQWAygTwVuHH/gfLf6s1bZVmqO4JqzqnTZyZTYw0IBCSAAgPCZleZEchQgTsPH83M7CmKoma7lavXlAXffOttBshk9nsgOl0UKLo8BBc1gdwUOBo9m5vbp4bANAu3tn5H2fHAwUO7j580V2AOBMITQIHhmbPHXAhkpcDR6JkeAtp2nrfz8JE6F5yb25dLJyBP2QRQoOz6EF3MBLJSoBoCOjOzx/AIxNr6HaVAhsbE3K+Tih0FJlVOkhFFIB8F6lt9hocfNre2lf9Wrl4TVSaCyZkACsy5+uTul0AmCrQ5t2sbJuO3ALQOgUkEUOAkQiyHQFcCOSjQxm02twm7MmY7CPQigAJ74WNjCBgIJK/A0eiZGuS5uLRs4KCGyczN7WsbJmPYlkUQ8EoABXrFS+NZE0hbgZbndjbDZLLuJSQ/KAEUOCh+dp40gbQV+OZbbxdFMTOzx/CQn35T2sb9B0mXmuRiJYACY60cccsnkLACz1+4qIZ3Gh6BqL0pTX69iDBDAigww6KTciACqSqQIaCBOhC78U8ABfpnzB5yJZCkAvUbXs5fuNhWWPOb0tq2Yj4EwhNAgeGZs8dcCKSnwN3HT9QQ0DfferutipbDZNo2Zz4EQhJAgSFps6+8CCSmQEu3MQQ0r14eebYoMPICEr5gAokpcHFpWQ0BNTzeZ/OmNMEVI7TsCKDA7EpOwsEIpKRAm3M7m2EyweCzIwjYEECBNpRYBwJdCCSjQBu32bwprQtEtoGATwIo0Cdd2s6bQBoKtPnCg+Wb0vLuDmQvkQAKlFgVYkqDQAIKtDm3sxwmk0ZNySIxAigwsYKSjiACsSvQ0m36NqHhTWmCqkIoEKgQQIEVGExCwCmB2BVo84UHmzelOYVKYxBwSQAFuqRJWxCoEohagfrczvAWUJthMlUgTENAGgEUKK0ixJMOgXgVaPOFB5s3paVTSzJJlAAKTLSwpCWAQKQKtPnCg82b0gRUgBAgMIEACpwAiMUQ6EwgRgUyBLRzudkwRgIoMMaqEXMcBKJToOUXHvTHcg1vSoujQkSZPQEUmH0XAIA3AnEpcNpHIAzDZLwRpWEIOCaAAh0DpTkIaAJxKZAhoLpwTORDAAXmU2syDU0gIgXafOHB5k1poRGzPwj0I4AC+/Fjawi0E4hFgTaP99kMk2knwRIICCWAAoUWhrASIBCFAm3cZnmbMIGSkUJuBFBgbhUn33AE5CvQ8gsP6mO5c3P7GAIarvewpyAEUGAQzOwkSwLCFWh5bmczTCbL8pJ0CgRQYApVJAeZBIQrULvN8IUHmzelyYRPVBCwIYACbSixDgS6EJCsQJsvPNi8Ka0LF7aBgBgCKFBMKQgkOQJiFcgQ0OT6Ggl1JIACO4JjMwhMJCBTgTZfeLB8U9pEAqwAAeEEUKDwAhFexAQEKtDyCw/qY7kHDh5iCGjE/Y/QLQigQAtIrAKBTgSkKZAhoJ3KyEYpE0CBKVeX3IYlIE2BNl94sHlT2rBU2TsEHBJAgQ5h0hQEfkBAlAL1IxCGLzzYDJP5QYb8AIHICaDAyAtI+IIJyFGgjdts3pQmGDahQaALARTYhRrbQMCGgBAF2nzhwfJNaTZZsw4EIiKAAiMqFqFGRiCwAjfuPzh1+kxtDKfNuZ3lMJnI6BMuBCwIoEALSKwCgU4EAitQjWSpPslg6Tab24SdALARBKQTQIHSK0R88RIIrED1PYeiKObm9qkxLzZfeLB5U1q8JSByCJgJoEAzH5ZCoDuBwAqcm9tXFMXMzB71v/oRCIaAdi8hW6ZOAAWmXmHyG45AYAUWz/8bjZ7p08GiKDbuP2gDYPOmtLZtmQ+BNAigwDTqSBYSCYRUoBr2eeDgIQVC3d5bvXmrjYvlm9LaNmc+BNIggALTqCNZSCQQUoHqw35vvvW2BmE4/7McJqObYgICqRJAgalWlryGJxBSgWpUy8rVazZp69uEho/l2rTDOhCInQAKjL2CxC+XQEgFqvt/hjM/jYlHIDQKJiCAAukDEPBFIKQC1UDQiWd1Nm9K84WDdiEgjwAKlFcTIkqFQDAF7j5+ooaDmsnZvCnN3AJLIZAYARSYWEFJRxCBYApUbltcWjYnr4bMFEWxuLRce4+aeUOWQiBVAigw1cqS1/AEgilQvRrt1OkzE3NeW7+jLpkeOHho4lXTia2xAgRiJ4ACY68g8cslEEyBE58CrDLSL86emdljeHFMdROmIZAqARSYamXJa3gCwRR44OChoig2t7Yn5rzz8NH5CxfVq9TU7UObQaQTm2UFCERKAAVGWjjCjoBAMAXqV6O1Qdl9/GT15i1lSrXyzMye8xcuchbYRoz5mRBAgZkUmjQHIBBGgepVn3Nz+5oZjkbP1tbvqAfhlfmKojh1+gxnfk1WzMmTAArMs+5kHYJAGAVu3H+gBnlWU1Kfz1UjX5T8FpeW19bvMBC0SolpCKBA+gAEfBEIo0A1HFS9Gq15q29ubt/qzVsM/vRVY9qNnAAKjLyAhC+YQBgFqlejLS4tc6tPcF8gNKEEUKDQwhBWAgTCKLA6vJNbfQl0G1IISQAFhqTNvvIiEECBo9EzbvXl1avI1ikBFOgUJ41BoEIggAJ3Hj7iVl8FOZMQmI4ACpyOF2tDwJ5AAAXaB8OaEIBAkwAKbDJhDgTcEECBbjjSCgS8EUCB3tDScPYEUGD2XQAA0gmgQOkVIr54CaDAeGtH5JkQQIGZFJo0ByCAAgeAzi4hMA0BFDgNLdaFwDQEUOA0tFgXAgMQQIEDQGeXmRBAgZkUmjTjJYAC460dkUsngAKlV4j4sieAArPvAgDwRgAFekNLwxBwQwAFuuFIKxBoEkCBTSbMgYAoAihQVDkIJikCKDCpcpJMigRQYIpVJadxBHYePlpcWg757/zpt79dP9L53+//4Z2Q0apv6o4jxzwIJEsABSZbWhKrEdjc2lYfVQj2v0tHftLZf9+uH1k5vRQsVLUj9d3dGjd+hEDCBFBgwsUltR8QQIEThYoCf9Bj+CEDAigwgyKT4nMCKBAF8qsAgRoBFFgDwo/JEkCBKDDZzk1iXQmgwK7k2C42AkIU+M0nr3165dTqBx+ufvDhr858tPrBh3c/Orv7T2807xpyLzC2Lka88RFAgfHVjIi7ERhcgd988tqF91Zeff13f7Z8u/nvnXd+8+i3P6+KEAV2KzRbQcCeAAq0Z8WacRMYVoGfXjnVJr+qDlc/+FBbEAXG3eGIPgYCKDCGKhGjCwIDKvDuR2ernjNPX3hvRVkQBbooO21AwEQABZrosCwlAkMp8N+vnTQ7r7lUnQuiwJS6H7nIJIACZdaFqNwTGESB33zy2k/fuNmU3MQ5j377cxTovhPQIgR+SAAF/pAHP6VLYBAF/suF8xNtN3aFMyd/jQLT7YxkJoUACpRSCeLwTWAQBR576x/HGs5m5t+f/tnEJ/ncrsDbYXx3QtqXRgAFSqsI8fgiEF6Bry/9tY3q2tY594szbg03sTUU6Kvz0a5UAihQamWIyzWB8Ar82bGftenNZv6pE383UVpuV0CBrjsd7UkngAKlV4j4XBFAgRN9iQJddTbaiYUACoylUsTZlwAKRIF9+xDbJ0cABSZXUhJqIYACUWBL12B2vgRQYL61zy3z8ApcOvITm3t+bev88r2TE6XldgUuhOb2S0G+KJA+kAuBQRR45uSv2wxnnv/q67/jucBcuiZ5DkcABQ7Hnj2HJTCIAju8HU2pcfWDD1Fg2A7C3nIkgAJzrHqeOQ+iwG/Xj7zzzm/MJ3zNpa++/rtvPnkNBebZUck6JAEUGJI2+xqSwFAK/OaT12w+k1QV4adXTn27fgQFDtld2HceBFBgHnUmy7IcSoHfrh959Nuf21vw7kdn+VgSHRYCYQigwDCc2cvwBAZU4LfrR7755LWJV0R/+sbN6ofjOQscvtMQQeoEUGDqFSa/7wiEV+C+v/jzldNL1X+/fO/k37z5q+o1TzX9V2/cOPeLM9U1V04vLR35idtnHia2xkMR33UW/j8XAigwl0qTZ3gFGpTz6l/+VP97+eUfGdYMuQgF8muSGwEUmFvF881XlAJDis1+Xygw31+PXDN3o8DNre2Vq9f4BwHJBE6dDv3tIXv3CFlzcWlZcgWJDQKKwNr6HVfKdqPAlavXhPwOEwYEIAABCKRNYHFpWZYCOQvkrzP5BDgLnHhY5CxQfjcmwpWr18SdBboSMu1AwB8B7gVOVCD3Av11P1qWScDNhVCZuREVBKoEUCAKrPYHpiFQliUKpBvkQgAFosBc+jp5WhNAgdaoWDFyAigQBUbehQnfPQEU6J4pLcokgAJRoMyeSVQDEkCBA8Jn10EJoEAUGLTDsbMYCKDAGKpEjC4IoEAU6KIf0UZSBFBgUuUkGQMBFIgCDd2DRXkSQIF51l1u1qPRs52Hj3zEhwKHUuDu4ye7j5/4qCltQqAnARTYEyCbuySwtn5nZmaPpwe0UeBQClTkV65eG42euewutAWB3gRQYG+ENOCCwObW9oGDh9QxGgVOdJWnFXyTn5nZ4/DVVi76HW3kTgAF5t4DBs9/9/GT2ts7fR+IPfkjgWbDkF9cWt7c2h684xEABHg7DH1gSAKj0bOVq9dmZvYoeZw6feb8hYtFUYQ5ECdgLOcpeCWv3sFdLTc3CIf89WPfzwlwFkhHGIbA2vqdubl96iC+uLSshsCor255PRA710ZKDXolrz5wMxo90yf96r4vNwiH+Q1kr88JoEA6QmgCOw8fLS4tK3PMze3buP9AR4AChxVqAAWqWhv6gO4MTEAgAAEUGAAyu3hBYOIZAArMRIGqQ4y9EsBvCwRCEkCBIWlnva/abb+x94FQYFYKLMuyeT+Y66JZHyaCJ48CgyPPb4cb9x9Ub/sZRgOiwNwUqH4bqqOC/T0Ymt9vHhlPJoACJzNijc4Edh8/0bf9bJ4JQ4F5KlB1sOqzoXNz+wx/KnXukGwIgRoBFFgDwo9uCIxGz9QTDuqYbvlmEBSYswJVz1NvCFIcFpeWx14wd9NHaQUCfDWePuCDwOrNW/rxrzffetv+KIYCUaC+QahRnL9wkRuEPn5PaZNH4+kDjgn0vJaFAvVxf5CJYA9F2HS73cdP3nzrbcVhZmbP6s1bNluxDgSmIsCF0KlwsXIrAScHLBQ4iPn0TkUpUHW1za1tPZbqwMFD3CBs/Q1kQScCKLATNjaqEFDj2vVhtM9lKxSoMQ4yIVCBqqN1vrRe6adMQmAMARQ4Bgqz7Am4HbygFHjq9JnNrW3n/1Zv3hrEKxHt1Ct59YI0+65VW7PbAKtaI/wIgRoBFFgDwo+2BDa3tvUDD7X3nNk20VhPKTAiZxCqPYGeClSdpfZmNT691PgdYsZ0BFDgdLxYuyxLfw8yr63fWVxazuTfSy+9VBTFK6/8OJN8z1+46OrXx/5lC672SDupEkCBqVbWS168zsoh1pdf/lFRFAx07IzU5pV7nRtnw0wIoMBMCu0gzdqf3urzRg7azbUJFNi/8s0Xr/dvkxayIoACsyp3x2S5AdMRnHEzFGjEM8VCH7elp9g9q8ZMAAXGXD3/sTf/yuY9Ha6oo0BXJFU7fHrJLc9MWkOBmRS6S5rca+lCzXobFGiNynZF7lXbkmK97wigwO9I8P8VArySowLD1yQK9ETWyYuKPMVGs9IIoEBpFRk4nmk/bzRwuDHvHgV6rV7P19V6jY3G5RBAgXJqMXAkvH0jcAFQYADgbt9eFCBgdhGYAAoMDFzo7qI+Urx/9lxRFPPz+x3C/fj22sLCYf3qk6PHjjtsXDXlQ4GuUBw9drwoCh9ZO8c4sUH+tpuIKOcVUGDO1f9T7glcL3J13NddQTWo/edJBihQAw8wwRX+AJBj3AUKjLFqbmJOZtSAWwV+9vmmkt+7J05+9fVulfX8/P6iKN4/e646s/N0egp0y6czWMOGjPMywMlzEQrMse4OP28kAZ9bBV66fKUoitnZvc3U3B7ifSiwGXO3Od0uhLrl0y1ym6349JINpUzWQYGZFPr7NNN7gtitAtXR/90TJ79H9t2U20O8HAV+fHttdnZv9Xpv2gosy5J3PnzXqXP/fxSYUQ9I9T1SbhVo8JxhUYduJESBH99e03c99fiX5BWo6sWb/zr028Q2QYGJFXR8Ov4+bzR+f2HnmhV4996GWkEd6Gdn916/sVq7yVeWpdKbloHlhHZGh4y7KVDloverT+DUHUozio9vrym3qezePXFSDXydnd17995GNYWaAq/fWK2Oj710+UoVoIGVjrPauLTp2vvfN7e2pUVIPP4IoEB/bKW0XHvPWXov+Ww77j8djapH/NqRunbQj1GBVbWbFR7OSycAAAw2SURBVGhGcenylVpn1Qps23B2du8XX+6orWpgqz9GoUCVRfK/JrUS8+OL3guIhAlk8udtmwL1icv1G6tPRyNV6C++3Hn3xEl1mK5ZUK1guNppWNShF/U8C6xewNTjVNtQ6JSv31jVoaqBP9VbgHqRVqDa8P2z5zTA6zdWFb3mg5hu+ehggk2kfbEkGMa4dsRZYFz1so02q5scY4/7+kitT1aq7NTS2dm9+siulxqO44ZFenP7iZ4KVMG8e+JkNcGxKLQsm8rXlD77fLMauVKgGiPz8e216qKyLPVWtQbd8qntNNiPqd4yDwYwrh2hwLjqNTnaDIe6NY/7T0cjdfiunvTU2KnjdXMFw3HcsKjWuM2PfRTYJqcmirIs1ZncwsLhsVGppGojYPUF5LaHIFUAtSuobvmMjTbYzPQGTgdDF9eOUGBc9ZoQbZ4PPDWP+3fvbaiLdc2TPE1Qnco0xWA4jhsW6WbtJ/ooUF/5rO2uiUKP9GmT2VhBagVWTzGr+1IXmWvidMunurtBphN7fHYQhvJ3igLl12iKCGs3/3YePppi42hXbR731V2upt6qKbZp0nAcNyyqtmw53VOBY+XURGGpwNqNPX0htC0XtUJttItbPm27Dja/eUEl2K7ZUTACKDAY6kA7yvCroc3jvh79UR2d2DZdc4nhOG5Y1KG6fRQ49uU1ZVk2UZRlqc7Y2v4gUEnVzueU4do2KcsyeQXWBojuPn7SocRsIp8ACpRfoy4RZjW2rXncVwfoNufV5tdGghg8Z1jUoUh9FFg7adN7b6Ioy9IwHEYvqo15GWs4vZe0FVi7lMJjgtW6pzeNAtOr6fcZZTK2rXncV2eBtct033MxThk8Z1hkbHL8wmAK1CeCRVFUh//ohyKaZ3t5KpCvSYzvqUnPRYFJl/d5clF/C9CmPE0F2twLbGvZ4DnDorbWDPNDKvDpaFQ799U/Liwcrr7qRQWcmwL5pqCho6a9CAWmXd8X2aU9tq2pwLahLjbFNnjOsMim5do6wRSoH+Orvefs6LHjteufOsKsFJjnOGpd68wnUGBGHSCZDwTWatZUoM1zgUoMzUcFDJ5Ti2ojR2rB2P8YRoFffLmjTvhqo37McfZRoCs+5gidLOXzgU4wRt0ICoy6fF2CT+/XvqnA6htMxh769Xdxq/fGFE2DAtXQyrahKNMWI7ACa4+xm6PtpkC3fMwR9lya6p+DPbFkuDkKzLDof0o5pYs/YxVYHQNy6fIV/Yz8V1/v6mEgY8fLGBSoN2yeO3boRmEUqN8Oo2/+VSfm5/dfv7F6/cZq7Q+Fbgp0y6cDUptN0r4pYEOAdaoEUGCVRl7TyQwBaFNg21cOlAPartcZFPh0NFInOlWL1J6psO9DwRR4995GM+xqCmr66LHj+g+Fbgp0y8eepP2ayQ8Ns0fBmooACsy9JyQwELxNgaq0d+9t1J6Uv35j1eAtgwJVg9dvrKo3ZCpzGJoy960wClS3POfn9zfjVOd/12+sKuEVRaH/LOimQLd8zPSmXbq5tX3g4CFVsrm5fTztNy3AVNdHgalWdrq8eBx4Ol4u1u6mwKn2rJ+FaBv5qVtTf0YURaFPBPWi2Ceyek1E7MUKHz8KDM9c7h55KVTI2gRQoB710zwFrGWqXxPTfEawtmZEP2b4ssCIqiMkVBQopBBSwmi+Gji9r8wLYR1AgfqJCH2Fsy13da247dWjbVtJns/XjiRXR05sKFBOLQRFUvvi7sb9B4KCSyWUAArUL/NUn4ZvPgGinh7Rg2XGrhAdb3pvdCUbMGAUOCB86bvm72ivFQqjwKejUW00UHMsqJoz8X6hVxpOGucahhOMWTWCArMq99TJcjdlamTWG4RRoArniy93qiM/qxZU40Kto5a7Iney5dZGcGQoUHBxxITGqzR8lCKkAn3EL6fN9F54JIdt8pGgwORL7CxBnqxyhvJ5QyiwP88EnmrtD4EW+hBAgX3o5bgt79dwVXUU2IdkMu826gOBbfsTQIH9GWbXgr+3LK6t31lcWs7k30svvVQUxSuv/DiTfM9fuOjqVyWlN9y6YkI73QigwG7c2Kr0cQ1q5eq16kgNplMisLi03P/Xhqvx/RnSQpUACqzSYHpqAm5HIigFnjp9ZnNrO/l/t3+/vnrz1oN/+zT5TFdv3iqKoqcCGZM19S8nG1gQQIEWkFhlEgFXF6aUAleuXpu0Q5bHRGBza7uPAv1deI8JIrH6IYAC/XDNr1UnwxNQYJIdp48CGX6VZJeQkxQKlFOLFCKpvZtqbf3OVFmhwKlwxbJyNwVubm0vLi2ru6Fzc/t4S18s5Y4rThQYV73iiLbzp5dQYBwFnjLKaRXI542mBMzq3QmgwO7s2NJMoMMLq1CgGWmkS+0VyAv5Ii1xvGGjwHhrF0HkzdcWm4NGgWY+kS61VGDt4sHOw0eR5kvYERFAgREVK9ZQ7W/qoMBYa2yMe6ICe95CNu6chRAwEUCBJjosc0jA5tNLKNAhcDlNGRTYvE7AJ5rlFC6HSFBgDlWWkuPEB7xQoJRSOY2jTYGuHid1GiyN5UUABeZVbwnZGl7zgQIlFMh5DE0Fun2pkPOAaTAfAigwn1rLynTsyx5RoKwiOYqmqkAfr5Z1FCbN5EgABeZYdTk51979cer0maIoeEGanAI5iUQp8MDBQ+cvXNQv/l65eo3bfk7w0kgfAiiwDz22dUCg9mY1FOiAqbAmlAK1/BaXlncfPxEWI+FkSgAFZlp4aWlXr49xFiitOj3j0Qqcm9u3ubXdszU2h4BDAijQIUya6ktAjZJAgX05Ctt+c2t7ZmbP6s1bwuIiHAiUKJBOII4AV8nElaRfQKPRM2779UPI1r4IoEBfZGkXAhCAAASEE0CBwgtEeBCAAAQg4IsACvRFlnYhAAEIQEA4ARQovECEBwEIQAACvgigQF9kaRcCEIAABIQTQIHCC0R4EIAABCDgiwAK9EWWdiEAAQhAQDgBFCi8QIQHAQhAAAK+CKBAX2RpFwIQgAAEhBNAgcILRHgQgAAEIOCLAAr0RZZ2IQABCEBAOAEUKLxAhAcBCEAAAr4IoEBfZGkXAhCAAASEE0CBwgtEeBCAAAQg4IsACvRFlnYhAAEIQEA4ARQovECEBwEIQAACvgigQF9kaRcCEIAABIQTQIHCC0R4EIAABCDgiwAK9EWWdiEAAQhAQDgBFCi8QIQHAQhAAAK+CKBAX2RpFwIQgAAEhBNAgcILRHgQgAAEIOCLAAr0RZZ2IQABCEBAOAEUKLxAhAcBCEAAAr4IoEBfZGkXAhCAAASEE0CBwgtEeBCAAAQg4IsACvRFlnYhAAEIQEA4ARQovECEBwEIQAACvgigQF9kaRcCEIAABIQTQIHCC0R4EIAABCDgiwAK9EWWdiEAAQhAQDgBFCi8QIQHAQhAAAK+CKBAX2RpFwIQgAAEhBNAgcILRHgQgAAEIOCLAAr0RZZ2IQABCEBAOAEUKLxAhAcBCEAAAr4IoEBfZGkXAhCAAASEE0CBwgtEeBCAAAQg4IsACvRFlnYhAAEIQEA4ARQovECEBwEIQAACvgigQF9kaRcCEIAABIQTQIHCC0R4EIAABCDgiwAK9EWWdiEAAQhAQDgBFCi8QIQHAQhAAAK+CKBAX2RpFwIQgAAEhBNAgcILRHgQgAAEIOCLAAr0RZZ2IQABCEBAOAEUKLxAhAcBCEAAAr4IuFHgytVrBf9BAAIQgAAE/BNYXFp2pUQU6L9c7AECEIAABNwREKdAV0KmHQhAAAIQgEAwAm7OAoOFy44gAAEIQAACrgigQFckaQcCEIAABCIjgAIjKxjhQgACEICAKwIo0BVJ2oEABCAAgcgIoMDICka4EIAABCDgigAKdEWSdiAAAQhAIDICKDCyghEuBCAAAQi4IoACXZGkHQhAAAIQiIwACoysYIQLAQhAAAKuCKBAVyRpBwIQgAAEIiOAAiMrGOFCAAIQgIArAijQFUnagQAEIACByAigwMgKRrgQgAAEIOCKAAp0RZJ2IAABCEAgMgIoMLKCES4EIAABCLgigAJdkaQdCEAAAhCIjAAKjKxghAsBCEAAAq4IoEBXJGkHAhCAAAQiI4ACIysY4UIAAhCAgCsCKNAVSdqBAAQgAIHICKDAyApGuBCAAAQg4IoACnRFknYgAAEIQCAyAv8P25M0vxY5uvoAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эпизод завершается при условиях:\n",
    "1) Угол наклона шеста становится больше $\\pm 12$ градусов или $\\pm 0,2095$ радиан\n",
    "2) Положение тележки больше $\\pm 2.4$\n",
    "3) Если количество шагов в эпизоде превышает 500 для версии v1 Cart Pole (200 для версии v0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Награда *Reward* в размере +1 начисляется за каждый пройденный шаг в рамках эпизода.\n",
    "Таким образом, более высокая сумма вознаграждений получается за более длительные эпизоды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим окружение *environment*\n",
    "\n",
    "render_mode=\"human\" означает, что мы хотим создать анимацию в отдельном окне.\n",
    "\n",
    "!!! render_mode='human' Создает визуализацию !!!\n",
    "\n",
    "*Вы также можете создать окружение без указания параметра render_mode. Это позволит создать окружение без создания анимации. Это полезно для отработки алгоритма обучения с подкреплением, поскольку генерация анимации во время обучения замедлит процесс обучения.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v1',render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройка matplotlib. Отображение графика внутри kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x27ad5b03d00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор процессора для работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем обучать агента Deep Q Learning (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать память воспроизведения опыта **replay memory** для обучения нашего DQN. В ней хранятся переходы *transitions *, которые наблюдает агент, что позволяет нам повторно использовать эти данные позже.\n",
    "\n",
    "**Transition** - именованный кортеж *named tuple*, представляющий один переход в нашей среде. По сути, он сопоставляет пары (state, action) с их результатом (next_state, reward). По факту это опыт *expirience*.\n",
    "\n",
    "**Replay Memory** - циклический буфер ограниченного размера, в котором хранятся недавно наблюдавшиеся переходы *transitions*. В нем также реализован метод **.sample()** для выбора случайного батча *batch* для обучения.\n",
    "\n",
    "``def sample`` - Выборка равна ``batch_size``, отправленному в эту функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition(state=2, action=3, next_state=1, reward=4)\n"
     ]
    }
   ],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определим модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель будет представлять собой нейронную сеть с *feed forward*, которая учитывает разницу между текущим и предыдущим обновлениями экрана. Она имеет два выхода: $Q(s, left)$ и $Q(s, right)$, где $s$ - вход нейронки. \n",
    "\n",
    "Инициализируем слои нейронки: вход ``self.layer1``, внутренний ``self.layer2``, выход ``self.layer3``\n",
    "\n",
    "Возвращает ``tensor([[left0exp,right0exp]...])``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BATCH_SIZE` - это количество переходов, выбранных из буфера воспроизведения\n",
    "\n",
    "``GAMMA`` - коэффициент дисконтирования\n",
    "\n",
    "``EPS_START`` - начальное значение epsilon\n",
    "\n",
    "``EPS_END`` - конечное значение epsilon\n",
    "\n",
    "``EPS_DECAY`` контролирует скорость экспоненциального затухания epsilon, чем выше, тем медленнее затухание\n",
    "\n",
    "``TAU`` - скорость обновления целевой сети (*rate of the target network*)\n",
    "\n",
    "``LR`` - скорость обучения оптимизатора `AdamW`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем получить основную информацию о нашей среде, используя эти строки кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим количество действий из пространства действий в *gym action space*\n",
    "\n",
    "Получим количество *state observations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``select_action`` - выберет действие в соответствии с политикой жадности epsilon. \n",
    "\n",
    "``plot_durations`` - вспомогательный инструмент для отображения графика продолжительности эпизодов, а также среднего значения за последние 100 эпизодов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    print(f'sample: {sample} / eps_threshold: {eps_threshold}')\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ``optimize_model`` выполняет один шаг оптимизации. Сначала он производит выборку batch, объединяет все тензоры в один, вычисляет\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t)\n",
    "$$ \n",
    "\n",
    "$$\n",
    "V(s_{t+1}) = max_a Q(s_{t+1},a)\n",
    "$$\n",
    "и объединяет их в нашу потерю.\n",
    "\n",
    "По определению, мы устанавливаем \n",
    "$$\n",
    "V(s) = 0\n",
    "$$\n",
    "если $s$ это конечное состояние.\n",
    "\n",
    "Мы также *target network* сеть для вычисления $V(s_{t_1}) = 0$ для дополнительной стабильности.\n",
    "\n",
    "*Target network* обновляется на каждом шаге с помощью программного обновления, управляемого гиперпараметром ``TAU``, который был определен ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже основной цикл обучения. \n",
    "\n",
    "В начале мы сбрасываем *reset* окружение и получаем тензор исходного состояния. \n",
    "\n",
    "Затем мы пробуем действие, выполняем его, наблюдаем за следующим состоянием и вознаграждением (всегда 1) и оптимизируем нашу модель один раз. \n",
    "\n",
    "Когда эпизод заканчивается (наша модель выходит из строя), мы перезапускаем цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод графика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
